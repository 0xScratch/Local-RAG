{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6c5652-493b-4c9b-8192-28b434e85f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5066d9-c3ab-4895-9f73-3a364db9cf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your LangSmith API key (optional):  Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "Enter your LangSmith Project Name (default = \"default\"):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54e3a67-5b76-4fde-8d16-560a7eeb6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma3:4b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a2878b-e028-4ace-9dd1-6fbd79b2ec11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bob! It's nice to meet you. ðŸ˜Š \\n\\nHow's your day going so far? Is there anything youâ€™d like to chat about, or were you just saying hello?\", additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-05-22T12:03:40.481920147Z', 'done': True, 'done_reason': 'stop', 'total_duration': 26718254642, 'load_duration': 16964922586, 'prompt_eval_count': 15, 'prompt_eval_duration': 3711028680, 'eval_count': 42, 'eval_duration': 6026016594, 'model_name': 'gemma3:4b'}, id='run--5daab4b9-c4bc-449c-b37c-ec99431c11ae-0', usage_metadata={'input_tokens': 15, 'output_tokens': 42, 'total_tokens': 57})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26712648-8da7-4e23-a35c-3e302da3ec08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no way of knowing your name! I don't have access to personal information. \\n\\nYou'll have to tell me your name. ðŸ˜Š \\n\\nWhat would you like me to call you?\", additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-05-22T05:46:44.108127301Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6295647916, 'load_duration': 63666087, 'prompt_eval_count': 15, 'prompt_eval_duration': 526910371, 'eval_count': 48, 'eval_duration': 5703974767, 'model_name': 'gemma3:4b'}, id='run--a362b33c-455f-4a19-8acd-1b7f7bfcce8c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 48, 'total_tokens': 63})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a8b7aba-a1ad-4e3a-b75a-2933612a77fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Bob! ðŸ˜Š \\n\\nIt's nice to meet you! \\n\\nDo you want to chat about something specific, or were you just curious?\", additional_kwargs={}, response_metadata={'model': 'gemma3:4b', 'created_at': '2025-05-22T12:03:48.660571004Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5428192271, 'load_duration': 138085700, 'prompt_eval_count': 41, 'prompt_eval_duration': 1229898040, 'eval_count': 34, 'eval_duration': 3962579000, 'model_name': 'gemma3:4b'}, id='run--11336ffa-5e92-41d2-a6d4-f0cf3434600b-0', usage_metadata={'input_tokens': 41, 'output_tokens': 34, 'total_tokens': 75})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6483f36-ddf3-47e2-a311-16e13720732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc0196c-deca-4176-8809-44b35c06819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a0f5bb-dfe1-4f64-93bf-7a67154872b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! It's nice to meet you. How's your day going so far? \n",
      "\n",
      "Do you want to chat about anything in particular, or were you just saying hello? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b576fc3-f548-48aa-9fb6-3be7b48865aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! ðŸ˜Š You told me that yourself! \n",
      "\n",
      "Just kidding (a little!). ðŸ˜‰ \n",
      "\n",
      "How are you doing today, Bob?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b8b3d7-7549-4257-981b-7b6bb8c17316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I donâ€™t know your name! As an AI, I have no memory of past conversations and don't know who you are. ðŸ˜Š \n",
      "\n",
      "You'll have to tell me your name!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b548c48-aa7b-4552-a872-a313a3dee124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Youâ€™re Bob! You started our conversation by saying â€œHi! Iâ€™m Bob.â€ ðŸ˜„\n",
      "\n",
      "Iâ€™m just a little bit of a persistent AI â€“ I keep reminding myself of what you told me! \n",
      "\n",
      "Howâ€™s your day going?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c81630-ac56-4126-81a1-8af9bfe9c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8837d871-0c53-4d21-8309-62b5748e413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59966cd-e898-45b3-8250-9485b2b76684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! A fine name for a seafaring lad! What brings ye to me today, savvy? Speak yer mind, and letâ€™s see if we canâ€™t find a bit oâ€™ treasure â€“ or at least a good yarn! ðŸ´â€â˜ ï¸\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6a3260-06be-4132-9dfd-7269e9f593cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Shiver me timbers! Ye be askin' a pirate what yer name is? Thatâ€™s a trick question, ainâ€™t it? \n",
      "\n",
      "Yer name be Jim! I just confirmed it with ye!  A clever lad, ye be! \n",
      "\n",
      "Now, what about *my* name? That's a secret I ainâ€™t sharing with just any landlubber!  Arrr!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f88b444-459b-4c5f-a1b0-f05b436d34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f6164d-a19c-47ea-a2ff-24f920504249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1a2638-79f8-42b9-9ece-90903cc5a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Â¡Hola Bob! Es un gusto conocerte. Â¿En quÃ© puedo ayudarte hoy? (Hello Bob! It's nice to meet you. How can I help you today?)\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32de69b-0208-4c88-be53-26715a9db57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. (Your name is Bob.) ðŸ˜Š \n",
      "\n",
      "Â¿CÃ³mo estÃ¡s? (How are you?)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c04205c-ef8c-473a-8a11-fa8b84da7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\chain\\Bootcamps\\Decentralized_AI_Bootcamp\\local-rag\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73789cc4-3033-4d79-8559-b5e22c6372b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\chain\\bootcamps\\decentralized_ai_bootcamp\\local-rag\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.2-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/10.5 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.5 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.3/10.5 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.5 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.5 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Installing collected packages: safetensors, regex, transformers\n",
      "\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 transformers-4.52.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0587ebc0-a495-4e98-aeda-6c387fd094a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2a6cb33-0322-48cd-8e4d-c991030cb163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! ðŸ˜Š \n",
      "\n",
      "Nice to meet you, Bob!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b60a69e9-e252-4392-b993-5b15d68a2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked me \"What math problem did I ask?\" ðŸ˜Š \n",
      "\n",
      "It was a little bit of a trick question! \n",
      "\n",
      "How are you doing today, Bob?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f7595c3-cd01-4c1e-a7d1-55f9bd01a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi| Todd|!|\n",
      "\n",
      "|Why| don|'|t| scientists| trust| atoms|?| |\n",
      "\n",
      "|...| Because| they| make| up| everything|!| ðŸ˜„| |\n",
      "\n",
      "|Hope| you| enjoyed| that|!| Would| you| like| to| hear| another| one|?||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaff35-5d6f-4b56-b821-22361bb5d74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
